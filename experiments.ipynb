{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "NotV2BEjb0fQ",
        "trpiKJAYch_w",
        "kOlpYCPncr4p"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installs, Imports and Loading the Model"
      ],
      "metadata": {
        "id": "NotV2BEjb0fQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install diffusers==0.6 transformers"
      ],
      "metadata": {
        "id": "YHkkK2wQalyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import autocast\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from typing import Callable, List, Optional, Union\n",
        "import inspect\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "DqZuOI_M2r9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", revision=\"fp16\", torch_dtype=torch.float16).to(\"cuda\")"
      ],
      "metadata": {
        "id": "9o_FUSfxzlCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir imgs"
      ],
      "metadata": {
        "id": "ife7PzEqcYGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir imgs/homonym_duplication imgs/meaning_edit imgs/meaning_sum"
      ],
      "metadata": {
        "id": "xiDRPCNUONCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function Definitions"
      ],
      "metadata": {
        "id": "o7auMmAqcIF7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vector Utility Functions"
      ],
      "metadata": {
        "id": "QFV9zoS9Wzl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def project(a, b):\n",
        "    bb_dotprod = torch.dot(b,b)\n",
        "    ab_dotprod = torch.dot(a,b)\n",
        "    if bb_dotprod != 0:\n",
        "        coeff = (ab_dotprod/bb_dotprod)\n",
        "    else:\n",
        "        coeff = 0\n",
        "    return coeff * b"
      ],
      "metadata": {
        "id": "i4tIXF3HcLRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def w_b(w, b):\n",
        "    v_b = torch.zeros((768)).type(torch.HalfTensor).cuda()\n",
        "    for j in range(len(b)):\n",
        "        v_b += torch.dot(w,b[j]) * b[j]\n",
        "    return v_b\n",
        "\n",
        "def normal(v):\n",
        "    return (1/torch.sqrt(torch.dot(v,v))) * v"
      ],
      "metadata": {
        "id": "SliJDCy1cPdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def norm(v):\n",
        "  return torch.sqrt(torch.dot(v,v))"
      ],
      "metadata": {
        "id": "phlBrxvRcRBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_sim(a,b):\n",
        "  return torch.dot(a,b)/(torch.sqrt(torch.dot(a,a))*torch.sqrt(torch.dot(b,b)))"
      ],
      "metadata": {
        "id": "h3ZVAQTGcTFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting Images\n",
        "\n",
        "Edited version of the ```StableDiffusionPipeline```'s ```__call__()``` function that enables giving the text embedding directly as input.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wxtgZ4LtW3Od"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_images(text_embeddings, pipe, img_name,prompt=None, negative_prompt=None,num_images_per_prompt=3):\n",
        "    height = 512\n",
        "    width = 512\n",
        "    num_inference_steps = 50\n",
        "    guidance_scale = 7.5\n",
        "    eta = 0.0\n",
        "    generator = None\n",
        "    latents = None\n",
        "    output_type=\"pil\"\n",
        "    return_dict = True\n",
        "    callback= None\n",
        "    callback_steps= 1\n",
        "    batch_size =1\n",
        "    with torch.no_grad():\n",
        "\n",
        "        bs_embed, seq_len, _ = text_embeddings.shape\n",
        "        text_embeddings = text_embeddings.repeat(1, num_images_per_prompt, 1)\n",
        "        text_embeddings = text_embeddings.view(bs_embed * num_images_per_prompt, seq_len, -1)\n",
        "\n",
        "        do_classifier_free_guidance = guidance_scale > 1.0\n",
        "        if do_classifier_free_guidance:\n",
        "            uncond_tokens: List[str]\n",
        "            if negative_prompt is None:\n",
        "                uncond_tokens = [\"\"]\n",
        "            elif type(prompt) is not type(negative_prompt):\n",
        "                raise TypeError(\n",
        "                    f\"`negative_prompt` should be the same type to `prompt`, but got {type(negative_prompt)} !=\"\n",
        "                    f\" {type(prompt)}.\"\n",
        "                )\n",
        "            elif isinstance(negative_prompt, str):\n",
        "                uncond_tokens = [negative_prompt]\n",
        "            elif batch_size != len(negative_prompt):\n",
        "                raise ValueError(\n",
        "                    f\"`negative_prompt`: {negative_prompt} has batch size {len(negative_prompt)}, but `prompt`:\"\n",
        "                    f\" {prompt} has batch size {batch_size}. Please make sure that passed `negative_prompt` matches\"\n",
        "                    \" the batch size of `prompt`.\"\n",
        "                )\n",
        "            else:\n",
        "                uncond_tokens = negative_prompt\n",
        "\n",
        "            max_length = text_embeddings.shape[1]\n",
        "            uncond_input = pipe.tokenizer(\n",
        "                uncond_tokens,\n",
        "                padding=\"max_length\",\n",
        "                max_length=max_length,\n",
        "                truncation=True,\n",
        "                return_tensors=\"pt\",\n",
        "            )\n",
        "            uncond_embeddings = pipe.text_encoder(uncond_input.input_ids.to(pipe.device))[0]\n",
        "\n",
        "            seq_len = uncond_embeddings.shape[1]\n",
        "            uncond_embeddings = uncond_embeddings.repeat(batch_size, num_images_per_prompt, 1)\n",
        "            uncond_embeddings = uncond_embeddings.view(batch_size * num_images_per_prompt, seq_len, -1)\n",
        "\n",
        "            text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
        "\n",
        "        latents_shape = (batch_size * num_images_per_prompt, pipe.unet.in_channels, height // 8, width // 8)\n",
        "        latents_dtype = text_embeddings.dtype\n",
        "        if latents is None:\n",
        "            if pipe.device.type == \"mps\":\n",
        "                latents = torch.randn(latents_shape, generator=generator, device=\"cpu\", dtype=latents_dtype).to(\n",
        "                    pipe.device\n",
        "                )\n",
        "            else:\n",
        "                latents = torch.randn(latents_shape, generator=generator, device=pipe.device, dtype=latents_dtype)\n",
        "        else:\n",
        "            if latents.shape != latents_shape:\n",
        "                raise ValueError(f\"Unexpected latents shape, got {latents.shape}, expected {latents_shape}\")\n",
        "            latents = latents.to(pipe.device)\n",
        "\n",
        "        pipe.scheduler.set_timesteps(num_inference_steps)\n",
        "\n",
        "        timesteps_tensor = pipe.scheduler.timesteps.to(pipe.device)\n",
        "\n",
        "        latents = latents * pipe.scheduler.init_noise_sigma\n",
        "\n",
        "        accepts_eta = \"eta\" in set(inspect.signature(pipe.scheduler.step).parameters.keys())\n",
        "        extra_step_kwargs = {}\n",
        "        if accepts_eta:\n",
        "            extra_step_kwargs[\"eta\"] = eta\n",
        "\n",
        "        for i, t in enumerate(pipe.progress_bar(timesteps_tensor)):\n",
        "            latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
        "            latent_model_input = pipe.scheduler.scale_model_input(latent_model_input, t)\n",
        "\n",
        "            noise_pred = pipe.unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
        "\n",
        "            if do_classifier_free_guidance:\n",
        "                noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "            latents = pipe.scheduler.step(noise_pred, t, latents, **extra_step_kwargs).prev_sample\n",
        "\n",
        "            if callback is not None and i % callback_steps == 0:\n",
        "                callback(i, t, latents)\n",
        "\n",
        "        latents = 1 / 0.18215 * latents\n",
        "        image = pipe.vae.decode(latents).sample\n",
        "\n",
        "        image = (image / 2 + 0.5).clamp(0, 1)\n",
        "\n",
        "        image = image.cpu().permute(0, 2, 3, 1).float().numpy()\n",
        "\n",
        "        has_nsfw_concept = None\n",
        "\n",
        "        if output_type == \"pil\":\n",
        "            image = pipe.numpy_to_pil(image)\n",
        "\n",
        "        if not return_dict:\n",
        "            print(\"NSFW\")\n",
        "\n",
        "        out=image\n",
        "\n",
        "        for i in range(len(image)):\n",
        "            image[i].save(\"imgs/\"+img_name + \"_\"+str(i)+\".png\")"
      ],
      "metadata": {
        "id": "d12Oz8wN3HI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting Encodings"
      ],
      "metadata": {
        "id": "hC9mR1nBXQG8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def one_prompt_embed(prompt_1, pipe):\n",
        "    text_inputs = pipe.tokenizer(\n",
        "        prompt_1,\n",
        "        padding=\"max_length\",\n",
        "        max_length=pipe.tokenizer.model_max_length,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    text_input_ids = text_inputs.input_ids\n",
        "\n",
        "    text_embeddings_1 = pipe.text_encoder(text_input_ids.to(pipe.device))[0]\n",
        "    \n",
        "    return text_embeddings_1"
      ],
      "metadata": {
        "id": "oPCUyBIE3EWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sum_embedding(prompt_1, prompt_2, pipe, weights=[0.5,0.5]):\n",
        "    text_inputs = pipe.tokenizer(\n",
        "        prompt_1,\n",
        "        padding=\"max_length\",\n",
        "        max_length=pipe.tokenizer.model_max_length,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    text_input_ids = text_inputs.input_ids\n",
        "\n",
        "    text_embeddings_1 = pipe.text_encoder(text_input_ids.to(pipe.device))[0]\n",
        "\n",
        "    text_inputs = pipe.tokenizer(\n",
        "        prompt_2,\n",
        "        padding=\"max_length\",\n",
        "        max_length=pipe.tokenizer.model_max_length,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    text_input_ids = text_inputs.input_ids\n",
        "\n",
        "    text_embeddings_2 = pipe.text_encoder(text_input_ids.to(pipe.device))[0]\n",
        "\n",
        "    text_embeddings = (weights[0] * text_embeddings_1) + (weights[1]*text_embeddings_2)\n",
        "\n",
        "    return text_embeddings"
      ],
      "metadata": {
        "id": "_4g_uPS23A01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate All Images for Experiments on Summing Encodings"
      ],
      "metadata": {
        "id": "D34U3V7bXTkg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def concept_sum(concept_1, concept_2, pipe, filename, weights=[0.5,0.5]):\n",
        "    for i in range(10):\n",
        "        get_images(sum_embedding(concept_1,concept_2, pipe,weights), pipe, filename+\"_sum_\"+str(i))\n",
        "    for i in range(10):\n",
        "        get_images(one_prompt_embed(concept_1, pipe), pipe, filename+\"_1_\"+str(i))\n",
        "        get_images(one_prompt_embed(concept_2, pipe), pipe, filename+\"_2_\"+str(i))"
      ],
      "metadata": {
        "id": "byk9Hk8w0vJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Find Meaning Directions"
      ],
      "metadata": {
        "id": "F7UrlQtXXa_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_svd(vectors_m, vectors_amb, n, model_dim=768):\n",
        "    mus = [torch.zeros((model_dim)).cuda() for i in range(n)]\n",
        "\n",
        "    for i in range(n):\n",
        "        mus[i] = (1/2)*(vectors_m[i]+vectors_amb[i])\n",
        "\n",
        "    subspace = torch.zeros((model_dim,model_dim)).cuda()\n",
        "\n",
        "    for i in range(n):\n",
        "        subspace += (1/2)*torch.outer(vectors_m[i] - mus[i],vectors_m[i]- mus[i])\n",
        "        subspace += (1/2)*torch.outer(vectors_amb[i]- mus[i],vectors_amb[i]- mus[i])\n",
        "\n",
        "    u_m, s_m, v = np.linalg.svd(subspace.detach().cpu(), full_matrices=True)\n",
        "    return torch.tensor(u_m).type(torch.HalfTensor).cuda(), s_m\n",
        "\n",
        "def find_vectors(w, sentences_1, sentences_2, sentences_amb, pipe, min_dim=20, threshold=0.9985, model_dim=768):\n",
        "    n = len(sentences_1)\n",
        "    vectors_1 = []\n",
        "    vectors_2 = []\n",
        "    vectors_amb = []\n",
        "    for i in range(n):\n",
        "        full_vec_1 = one_prompt_embed(sentences_1[i], pipe)\n",
        "        w_idx = sentences_1[i].split(\" \").index(w) + 1\n",
        "        vec_1 = full_vec_1[:,w_idx,:].squeeze(0)\n",
        "        vectors_1.append(vec_1)\n",
        "\n",
        "        full_vec_2 = one_prompt_embed(sentences_2[i], pipe)\n",
        "        w_idx = sentences_2[i].split(\" \").index(w) + 1\n",
        "        vec_2 = full_vec_2[:,w_idx,:].squeeze(0)\n",
        "        vectors_2.append(vec_2)\n",
        "\n",
        "        full_vec_amb = one_prompt_embed(sentences_amb[i], pipe)\n",
        "        w_idx = sentences_amb[i].split(\" \").index(w) + 1\n",
        "        vec_amb = full_vec_amb[:,w_idx,:].squeeze(0)\n",
        "        vectors_amb.append(vec_amb)\n",
        "\n",
        "    u_1, s_1 = get_svd(vectors_1, vectors_amb, n, model_dim)\n",
        "    u_2, s_2 = get_svd(vectors_2, vectors_amb, n, model_dim)\n",
        "\n",
        "    dim = 0\n",
        "    while dim < min_dim or sum(s_1[:dim])/sum(s_1) < threshold or sum(s_2[:dim])/sum(s_2) < threshold:\n",
        "        dim += 1\n",
        "\n",
        "    v_1 = torch.zeros((model_dim)).type(torch.HalfTensor).cuda()\n",
        "    for j in range(dim):\n",
        "        all_vals = [torch.dot(vectors_1[i] - vectors_amb[i], u_1[:,j]) for i in range(n)]\n",
        "        all_vals.sort()\n",
        "        v_1 += all_vals[n//2] * u_1[:,j]\n",
        "    norm_v_1 = norm(v_1)\n",
        "    v_1 = normal(v_1)\n",
        "\n",
        "    v_2 = torch.zeros((model_dim)).type(torch.HalfTensor).cuda()\n",
        "    for j in range(dim):\n",
        "        all_vals = [torch.dot(vectors_2[i] - vectors_amb[i], u_2[:,j]) for i in range(n)]\n",
        "        all_vals.sort()\n",
        "        v_2 += all_vals[n//2] * u_2[:,j]\n",
        "    norm_v_2 = norm(v_2)\n",
        "    v_2 = normal(v_2)\n",
        "\n",
        "    return v_1, v_2"
      ],
      "metadata": {
        "id": "XWMTr7MnbtYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Editing Embedding"
      ],
      "metadata": {
        "id": "SoVdyJ6-Xd7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def edit_embed(orig_embed, meaning_1, meaning_2):\n",
        "    # away from meaning_1, towards meaning_2\n",
        "    dot_1 = torch.abs(torch.dot(orig_embed, meaning_1))\n",
        "    dot_2 = torch.abs(torch.dot(orig_embed, meaning_2))\n",
        "\n",
        "    orig_embed = orig_embed - project(orig_embed, meaning_2) + (dot_1 +dot_2) * (meaning_2)\n",
        "    return orig_embed"
      ],
      "metadata": {
        "id": "xZhNklLctB7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate All Images for Sense Editing Experiments"
      ],
      "metadata": {
        "id": "JNawHZp0Xkpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def edit_prompts(word, prompt_dict, sentences_1, sentences_2, sentences_amb, pipe, repeat=5):\n",
        "    v_1, v_2 = find_vectors(word, sentences_1, sentences_2, sentences_amb, pipe)\n",
        "    for prompt, filename in prompt_dict.items():\n",
        "        orig_prompt = prompt\n",
        "        orig_embed = one_prompt_embed(orig_prompt,pipe)\n",
        "        idx = orig_prompt.split(\" \").index(word) + 1\n",
        "\n",
        "        embed_1 = orig_embed.detach().clone()\n",
        "        embed_1[:,idx,:] = edit_embed(embed_1[:,idx,:].squeeze(0).clone(), v_2, v_1).clone()\n",
        "\n",
        "        embed_2 = orig_embed.detach().clone()\n",
        "        embed_2[:,idx,:] = edit_embed(embed_2[:,idx,:].squeeze(0).clone(), v_1, v_2).clone()\n",
        "\n",
        "        for i in range(repeat):\n",
        "            get_images(embed_1, pipe, filename + \"sense_1_\" + str(i))\n",
        "            get_images(embed_2, pipe, filename + \"sense_2_\" + str(i))\n",
        "            get_images(orig_embed, pipe, filename + \"amb_\" + str(i))"
      ],
      "metadata": {
        "id": "sofKVKNQXCD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiments"
      ],
      "metadata": {
        "id": "kOlpYCPncr4p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Homonym Duplication\n",
        "\n",
        "Note: Homonym duplication is rare in Stable Diffusion, so it may not necessarily occur in any of the generated images"
      ],
      "metadata": {
        "id": "NczXdgEhV19J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "    get_images(one_prompt_embed(\"a woman with a silk bow and arrow\", pipe), pipe, \"homonym_duplication/dup_bow_\"+str(i))"
      ],
      "metadata": {
        "id": "OTgRJtSSPPDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "    get_images(one_prompt_embed(\"tall cranes by the ocean\",pipe), pipe, \"homonym_duplication/dup_crane_\"+str(i))"
      ],
      "metadata": {
        "id": "XG3tNbP4TJ7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "    get_images(one_prompt_embed(\"a crane by the ocean\",pipe), pipe, \"homonym_duplication/dup_crane_sea_\"+str(i))"
      ],
      "metadata": {
        "id": "oAOzAqJwzrOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    get_images(one_prompt_embed(\"a bat and a baseball fly through the air\",pipe), pipe, \"homonym_duplication/neg_dup_bat_\"+str(i),prompt=\"a bat and a baseball fly through the air\", negative_prompt=\"disfigured, deformed, bad anatomy, low quality, jpeg artifacts\")"
      ],
      "metadata": {
        "id": "Gjj1iIjjZ_fH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "    get_images(one_prompt_embed(\"a man with glasses\",pipe), pipe, \"homonym_duplication/dup_glasses_\"+str(i))"
      ],
      "metadata": {
        "id": "AzXygLn4z5ol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "    get_images(one_prompt_embed(\"a gentleman with a bow and arrow\",pipe), pipe, \"homonym_duplication/dup_bow_gent_\"+str(i))"
      ],
      "metadata": {
        "id": "LU1i5fgK-a5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "    get_images(one_prompt_embed(\"a baseball bat inside a spooky cave\",pipe), pipe, \"homonym_duplication/dup_bat_cave_\"+str(i))"
      ],
      "metadata": {
        "id": "Nrqq9eraDW7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summing Encodings"
      ],
      "metadata": {
        "id": "8bd3RyL4c1ag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "concept_sum(\"tree\", \"cat\", pipe, \"meaning_sum/treecat\")\n",
        "concept_sum(\"dog\", \"lake\", pipe, \"meaning_sum/doglake\")\n",
        "concept_sum(\"bear\", \"waterfall\", pipe, \"meaning_sum/bearwaterfall\")\n",
        "concept_sum(\"bear\", \"hat\", pipe, \"meaning_sum/bearhat\")\n",
        "concept_sum(\"a wall painted red\", \"a wall painted blue\", pipe, \"meaning_sum/redbluewall\")\n",
        "concept_sum(\"a completely black cat\", \"a completely white cat\", pipe, \"meaning_sum/blackwhitecat\")"
      ],
      "metadata": {
        "id": "UDYDVmR91PqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Editing Meaning"
      ],
      "metadata": {
        "id": "eUMfFlEneKWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "crane_sentence_animal = [\"a flying crane\", \"there is a flying crane\", \n",
        "                    \"there is a hungry crane on the nature reserve\", \n",
        "                    \"a hungry crane hunts fish\", \n",
        "                    \"a boy feeds a hungry crane\", \n",
        "                    \"a feathered crane beside a nest\", \n",
        "                    \"a hungry crane is eating some fish\", \n",
        "                    \"a feathered crane in a nest\"]\n",
        "\n",
        "crane_sentence_construction = [\"a tower crane\", \"there is a tower crane\", \n",
        "                    \"there is a tower crane on the building site\", \n",
        "                    \"a tower crane lifts loads\", \n",
        "                    \"a man operates a tower crane\", \n",
        "                    \"a tower crane beside a bulldozer\", \n",
        "                    \"a tower crane is lifting a container\", \n",
        "                    \"a tower crane in a quarry\"]\n",
        "\n",
        "crane_sentence_amb = [\"a crane\", \"there is a crane\", \n",
        "                    \"there is a crane on the other side\", \n",
        "                    \"a crane is tall\", \n",
        "                    \"a boy sees a crane\", \n",
        "                    \"a crane beside a tree\", \n",
        "                    \"a crane is casting a shadow\", \n",
        "                    \"a crane by the ocean\"]\n",
        "\n",
        "bat_sentence_baseball = [\"a baseball bat\", \"there is a baseball bat\", \n",
        "                    \"i play baseball with the bat\", \n",
        "                    \"to play baseball you need a bat and a ball\", \n",
        "                    \"the boy bought a baseball bat\", \n",
        "                    \"a baseball player throws a baseball bat\", \n",
        "                    \"a baseball bat is laying on the base\", \n",
        "                    \"a baseball bat in the store\",\n",
        "                    \"a sports store sells a baseball bat\"]\n",
        "\n",
        "bat_sentence_animal = [\"a vampire bat\", \"there is a vampire bat\", \n",
        "                    \"i feed insects to the vampire bat\", \n",
        "                    \"to celebrate halloween you need a vampire bat and a pumpkin\", \n",
        "                    \"the boy saw a vampire bat\", \n",
        "                    \"a wildlife expert feeds a vampire bat\", \n",
        "                    \"a vampire bat is hanging from the tree\", \n",
        "                    \"a vampire bat in the cave\",\n",
        "                    \"a special zoo keeps a vampire bat\"]\n",
        "\n",
        "bat_sentence_amb = [\"a bat\",\"there is a bat\", \"i do things with the bat\", \n",
        "                \"to do anything you need a bat and something else\", \n",
        "                \"the person saw a bat\",\n",
        "                \"a person mentions a bat\",\n",
        "                \"a bat is rolling on the floor\",\n",
        "                \"a bat in the place\",\n",
        "                \"a location has a bat\"]"
      ],
      "metadata": {
        "id": "QUeFxXg6qT0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "edit_prompts(\"bat\", {\"a bat\":\"meaning_edit/bat_\", \"a bat and a baseball fly through the air\":\"meaning_edit/bat_fly_through_the_air_\"}, bat_sentence_baseball, bat_sentence_animal, bat_sentence_amb, pipe)\n",
        "edit_prompts(\"crane\", {\"a crane\":\"meaning_edit/crane_\", \"a crane by the ocean\":\"meaning_edit/crane_by_ocean_\",\"a crane surrounded by nature\":\"meaning_edit/crane_nature_\"}, crane_sentence_construction, crane_sentence_animal, crane_sentence_amb, pipe)"
      ],
      "metadata": {
        "id": "VnJhp2x7qdSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zip Images to Download"
      ],
      "metadata": {
        "id": "GV5-1s3oWgMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r imgs.zip imgs/ "
      ],
      "metadata": {
        "id": "Uj6xUJmWHu_4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}